<div align="center">

# Extreme Precipitation Downscaling with Wasserstein Regularized Diffusion
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
<!---
[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)
-->
</div>

## Dependencies
To create a conda environment with all the required packages, run:
```
conda env create -f environment.yml
```

## Dataset Compilation
Instructions on how to obtain required training and validation data: 
### CPC Unified Gauge-Based Analysis of Daily Precipitation
Navigate to https://psl.noaa.gov/data/gridded/data.cpc.globalprecip.html, download `.nc` under precipitation.
Choose appropriate years.
The gauge density files are stored seperately on NOAA's FTP server: https://ftp.cpc.ncep.noaa.gov/precip/CPC_UNI_PRCP/.
Download `.gz` files from https://ftp.cpc.ncep.noaa.gov/precip/CPC_UNI_PRCP/GAUGE_CONUS/RT/

### ERA5 and MRMS
Instructions can be found on this [repository](https://github.com/dossgollin-lab/climate-data).

### Prepare datasets
Once all the data are downloaded, navigate to `config.cpc_mrms_data.yaml` and update these entries according to 
where the data are stored on your local machine: 
 - `data.base_path`
 - `data.dataset_path`
   - `mrms`
   - `cpc`
   - `era5`
   - `cpc_gauge`
 
   
## Training
To train the proposed model (WassDiff), run `python train_cpc.py` using configuration file `configs/downscale_cpc_density.yaml`.
The `main()` function loads this config file by default.

To train the baseline diffusion model (SBDM, to be trained without Wasserstein Distance Regularization), use the same config file 
as above but with additional arguments: 
```
  python train_cpc.py training.use_emd=false training.emd_weight=0
```

To train the CNN baseline, run `python train_cnn.py` using configuration file `configs/baseline_cnn.yaml`.

The CPC_Int (bilinearly interpolated CPC) is automatically generated by the dataloader 
`precipitation/cpc_mrms_loader.py`

## Evaluation

Modeel weights for all three models can be found on 
[Google Drive](https://drive.google.com/drive/folders/1mVHRyGTJDVZ_iS_yV0jVxmQs3bOkEoyB?usp=share_link).

Quantitative evaluation can be done by running `python eval_val_set.py` for WassDiff, SBDM, CPC_Int and 
`python eval_val_set_baselines.py` for CNN.

To generate sample (single or ensemble) on a specified region and date (input data required),
run `python specify_eval.py`, and adjust `lon`, `lat`, and `date` parameters accordingly.